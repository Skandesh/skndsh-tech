---
title: "THE ATTENTION ECONOMY: BUILDING A MIND THAT READS IN PARALLEL"
date: "2024.12.25"
category: AI / NEURAL NETWORKS
readTime: "18 MIN"
---

# THE COLD CASE

&quot;The bank robber wore a mask.&quot;

Which bank? The building with vaults and tellers? Or the muddy edge of a river?

You don&apos;t know. You can&apos;t know. Not from this sentence alone.

But add context: &quot;He sprinted down to the river. The bank robber wore a mask.&quot;

Now you know. The word &quot;bank&quot; hasn&apos;t changed. Its letters are identical. What changed is that your brain looked at &quot;river&quot; and made a connection. You paid attention to the right word at the right moment, and meaning crystallized.

This is the fundamental problem of language understanding. Words don&apos;t mean anything in isolation. They mean something because of the other words around them. And for decades, machines couldn&apos;t do what you just did.

# THE OLD DETECTIVE

Before 2017, the dominant approach to language was the recurrent neural network. Think of it as a detective who examines evidence one piece at a time, left to right, keeping notes as he goes.

The process works like this: The detective reads the first word, writes something in his notebook. Reads the second word, updates his notes based on what he just learned. Reads the third word, updates again. By the time he reaches the end of a sentence, his notebook contains a summary of everything he&apos;s seen.

The problem is that notebooks have limited space. By the time the detective reaches word fifty, word one is a faded memory. The notes have been overwritten forty-nine times. Critical details are lost in the shuffle.

This is the vanishing gradient problem. Information decays as sequences get longer. The detective forgets.

Consider this sentence: &quot;The cat that the dog that the man bought chased ran away.&quot;

By the time an RNN reaches &quot;ran away,&quot; it has lost track of what ran. Was it the cat? The dog? The man? The structure is too nested, the dependencies too long. The RNN gets lost in its own notes.

A human doesn&apos;t get lost. You can trace back through the sentence and figure out that the cat ran away. And now, a transformer can too.

# THE INSIGHT

The breakthrough came from a simple question: What if the detective didn&apos;t have to examine evidence one piece at a time? What if he could see the entire crime scene at once and draw connections between any two pieces of evidence directly?

Forget the notebook. Imagine an investigation board instead.

Every piece of evidence—every word—is pinned to the board. Red strings connect related evidence. Some strings are thick, representing strong connections. Others are thin, barely there. The pattern of strings IS the understanding.

The transformer&apos;s revolutionary idea is to compute all these connections in parallel. Not word by word. All at once. Every word can look at every other word, simultaneously, and decide what&apos;s relevant.

No fading memory. No lost details. The entire context is always available.

# THE INTERROGATION

Here&apos;s how a word figures out what to pay attention to.

Every word in a sentence asks three questions simultaneously. These aren&apos;t metaphorical—they&apos;re actual mathematical projections computed by the network.

First, the word generates a Query. This is the word asking: &quot;What am I looking for? What information do I need to understand myself in context?&quot;

Second, the word generates a Key. This is the word advertising: &quot;Here&apos;s what I offer. Here&apos;s the kind of information I contain.&quot;

Third, the word generates a Value. This is the actual semantic content: &quot;If you decide I&apos;m relevant to you, this is the information you&apos;ll take from me.&quot;

Now the matching begins. Every word&apos;s Query is compared against every other word&apos;s Key. The comparison uses a dot product—a measure of how well two vectors point in the same direction. High dot product means high compatibility. Low dot product means irrelevance.

These compatibility scores get normalized through a function called softmax. This converts raw scores into percentages that sum to one. Now each word has a probability distribution over all other words, representing how much attention to pay to each.

Finally, each word computes a weighted sum of everyone&apos;s Values, using those attention percentages as weights. The result is a new representation of the word—one that incorporates context from the entire sentence.

The word &quot;bank&quot; started out ambiguous. But after attending to &quot;river&quot; (high attention) and &quot;robber&quot; (lower attention), the new representation of &quot;bank&quot; now carries the meaning of a riverbank. The ambiguity is resolved. Context flows through the attention mechanism.

# THE EXAMPLE

Let&apos;s trace through a real sentence.

&quot;The animal didn&apos;t cross the street because it was too tired.&quot;

What does &quot;it&quot; refer to? The animal or the street?

Your intuition says the animal. Streets don&apos;t get tired. But how does a transformer reach the same conclusion?

The word &quot;it&quot; generates a Query that essentially asks: &quot;What noun am I referring to?&quot;

The word &quot;animal&quot; has a Key that matches well: it&apos;s a noun, it&apos;s animate, it can experience states like tiredness.

The word &quot;street&quot; has a Key that matches poorly: it&apos;s a noun, yes, but it&apos;s inanimate. Streets don&apos;t have internal states.

The attention score from &quot;it&quot; to &quot;animal&quot; is high. The attention score from &quot;it&quot; to &quot;street&quot; is low. When &quot;it&quot; computes its weighted sum of Values, it inherits semantic meaning primarily from &quot;animal.&quot;

Now change the sentence: &quot;The animal didn&apos;t cross the street because it was too wide.&quot;

Same structure. Same pronoun. But &quot;wide&quot; is a property of streets, not animals. Now the Keys flip. &quot;Street&quot; matches better. The attention flows differently. &quot;It&quot; now refers to the street.

Same words, different attention patterns, different understanding. This dynamic routing of information based on semantic compatibility is why transformers work.

# THE TEAM

One detective with one perspective isn&apos;t enough. What if you had a team?

Detective One notices physical descriptions—colors, sizes, shapes.
Detective Two notices actions—verbs, movement, cause and effect.
Detective Three notices time—when things happen, sequences, before and after.
Detective Four notices relationships—who owns what, who said what to whom.

This is multi-head attention. The transformer runs multiple attention mechanisms in parallel, each with its own set of Query, Key, and Value projections. Each &quot;head&quot; learns to focus on different types of relationships.

One head might specialize in subject-verb connections. Another in adjective-noun pairs. Another in pronoun resolution. Another in long-range dependencies that span entire paragraphs.

Nobody programs these specializations. They emerge from training. The model discovers that different types of linguistic connections require different lenses.

After all heads compute their weighted sums, the results are concatenated and projected back into a single representation. Multiple perspectives fused into one rich understanding.

# THE BLIND SPOT

There&apos;s a critical flaw in what I&apos;ve described so far. Pure attention treats the input as a set, not a sequence.

&quot;John loves Mary&quot; and &quot;Mary loves John&quot; would produce identical attention patterns. Same words. Same pairwise relationships. But radically different meanings.

The attention mechanism doesn&apos;t inherently know that John comes before loves, which comes before Mary. It sees all words simultaneously, without order.

The fix is positional encoding. Before attention happens, each word gets position information added to its representation. Word one receives a specific numerical pattern. Word two receives a slightly different pattern. Word one hundred receives a very different pattern.

The original transformer used an elegant trick: sine and cosine waves of different frequencies. Think of it like giving each position a unique fingerprint made of overlapping waves.

Why waves? Because nearby positions have similar encodings—position five is closer to position four than to position fifty. And because the model can learn to reason about relative positions. &quot;Three words back&quot; becomes a learnable concept.

Now &quot;John&quot; at position one is distinguishable from &quot;John&quot; at position five. Order is preserved. &quot;John loves Mary&quot; and &quot;Mary loves John&quot; finally look different to the model.

# THE CASE FILE

One round of attention isn&apos;t enough.

Imagine the first pass through attention as establishing basic connections. &quot;The&quot; connects to &quot;cat.&quot; &quot;Big&quot; connects to &quot;cat.&quot; Subject-verb links form.

The second pass takes these enriched representations and applies attention again. Now &quot;the big cat&quot; behaves as a unit. Phrases emerge from words.

The third pass finds clause structure. Subject-verb-object patterns. Nested dependencies.

The fourth pass and beyond find increasingly abstract relationships. Intent. Sentiment. Implications. Sarcasm. The model builds up from tokens to concepts.

Modern language models stack dozens or even hundreds of these layers. GPT-3 has 96 layers. GPT-4 reportedly has around 120. Each layer refines the representations further, finding patterns the previous layers missed.

Depth is where reasoning lives. You can&apos;t understand &quot;The politician&apos;s promise to reduce taxes was, as usual, wildly optimistic&quot; in a single pass. It takes multiple iterations to recognize that &quot;as usual&quot; implies a pattern of behavior, that &quot;wildly optimistic&quot; is probably ironic given context, that the sentence is actually a criticism disguised as neutral observation.

Transformers learn to do this by stacking layers. Each layer asks: given what I now understand, what else can I figure out?

# THE REAL WORLD

Where do transformers run today? Everywhere that language matters.

GPT-4, Claude, and Gemini are transformers. Every conversation you have with a chatbot, every code completion you accept, every summary you generate—it&apos;s attention all the way down.

Google Search uses BERT—a transformer—to understand what you&apos;re actually asking when you type a query. Not just keyword matching. Semantic understanding.

DALL-E and Stable Diffusion use transformers. The text encoder that interprets your prompt (&quot;a cat wearing a top hat, oil painting&quot;) is a transformer. Even the image generation process itself uses attention mechanisms borrowed from the same architecture.

Whisper, the speech recognition system, is a transformer. GitHub Copilot is a transformer. Translation services, document summarizers, content moderators—transformers.

Perhaps most surprising: AlphaFold, the system that solved protein structure prediction, uses attention. Amino acids attend to other amino acids across the protein chain, figuring out which ones interact in three-dimensional space. The architecture invented for language works for biology.

The 2017 paper was titled &quot;Attention Is All You Need.&quot; It wasn&apos;t hyperbole. That paper obsoleted an entire generation of neural network architectures. RNNs, LSTMs, GRUs—they still exist, but transformers have replaced them in almost every high-stakes application.

# THE MENTAL MODEL

When someone asks you how transformers work, here&apos;s the image to hold in your mind.

A detective&apos;s investigation board. Cork surface. Harsh lighting. Dozens of clues pinned up—photographs, notes, names, dates. Red strings connect related items. The thickness of each string shows how relevant one clue is to another.

Now imagine every clue on that board is asking a question: &quot;Who here can help me understand myself better?&quot;

The questions broadcast out. Each clue advertises what it offers. Compatibility is measured. Strings form between relevant pairs.

But there isn&apos;t just one board. Eight boards run in parallel. One detective looks for alibis. Another looks for motives. Another tracks the timeline. Another maps relationships. Each board finds different patterns in the same evidence.

The boards stack. Layer by layer, the investigation deepens. First pass: who was where. Second pass: who had motive. Third pass: who had opportunity. Fourth pass: who had both. The case builds.

By the end, each piece of evidence knows exactly what it means in context. The clue that seemed random now connects to three others. The word that was ambiguous now has a clear referent. The sentence that could mean many things now means one specific thing.

The case is solved. The meaning is found. That&apos;s attention. That&apos;s transformers. That&apos;s how machines learned to understand language.

---

*Head to the Lab to see attention in action. Type any sentence. Watch the strings form between words. See which words attend to which. Toggle between attention heads. The investigation board awaits.*
